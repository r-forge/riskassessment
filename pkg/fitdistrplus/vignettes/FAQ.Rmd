---
title: Frequently Asked Questions
author: Marie Laure Delignette Muller, Christophe Dutang
date: '`r Sys.Date()`'
output:
  html_vignette:
    toc: yes
    number_sections: yes
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Frequently Asked Questions} 
  %!\VignetteEncoding{UTF-8}
  \usepackage[utf8]{inputenc}
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
require(fitdistrplus)
set.seed(1234)
```

# Questions regarding distributions

## How do I know the root name of a distribution?
The root name of a probability distribution is the `name` which is used the `d`, `p`, `q`, `r` functions. For base R distributions, the root names are given in R-intro : https://cran.r-project.org/doc/manuals/R-intro.html#Probability-distributions.
For example, you must use `"pois"` for the Poisson distribution and **not** `"poisson"`.

## How do I find "non standard" distributions?

For non-standard distributions, you can either find a package
implementing them or define by yourself. 
A comprehensive list of non-standard distributions is given in the Distributions task view
https://cran.r-project.org/web/views/Distributions.html.
Here are some two examples of user-defined distributions. A third example (shifted exponential) is given in FAQ 3.5.4.

* The Gumbel distribution
```{r, eval=FALSE}
dgumbel <- function(x, a, b) 1/b*exp((a-x)/b)*exp(-exp((a-x)/b))
pgumbel <- function(q, a, b) exp(-exp((a-q)/b))
qgumbel <- function(p, a, b) a-b*log(-log(p))
data(groundbeef)
fitgumbel <- fitdist(groundbeef$serving, "gumbel", start=list(a=10, b=10))
```

* The zero-modified geometric distribution
```{r, eval=FALSE}
dzmgeom <- function(x, p1, p2) p1 * (x == 0) + (1-p1)*dgeom(x-1, p2)
pzmgeom <- function(q, p1, p2) p1 * (q >= 0) + (1-p1)*pgeom(q-1, p2)
rzmgeom <- function(n, p1, p2) 
{
  u <- rbinom(n, 1, 1-p1) #prob to get zero is p1
  u[u != 0] <- rgeom(sum(u != 0), p2)+1
  u
}
x2 <- rzmgeom(1000, 1/2, 1/10)
fitdist(x2, "zmgeom", start=list(p1=1/2, p2=1/2))
```


## How do I set (or find) initial values for non standard distributions?
As documented, we provide initial values for the following distributions:
`"norm"`, `"lnorm"`, `"exp"`, `"pois"`, `"cauchy"`, `"gamma`", `"logis"`, `"nbinom"`, `"geom"`, `"beta"`, `"weibull"` from the `stats` package; `"invgamma"`, `"llogis"`, `"invweibull"`, `"pareto1"`, `"pareto"` from the `actuar` package.

Look first at statistics and probability books such as 

* different volumes of N. L. Johnson, S. Kotz and N. Balakrishnan books, e.g. **Continuous Univariate Distributions, Vol. 1**,  
* **Thesaurus of univariate discrete probability distributions** by G. Wimmer and G. Altmann.  
* **Statistical Distributions** by M. Evans, N. Hastings, B. Peacock.  
* **Distributional Analysis with L-moment Statistics using the R Environment for Statistical Computing** by W. Asquith.  

If not available, find initial values by equalling theoretical and empirical quartiles.
You may also consider the `prefit()` function to find initial values.

## Is it possible to fit a distribution with at least 3 parameters?
Yes, an example with the Burr distribution is detailed in the JSS paper. We reproduce it very quickly here.
```{r, message=FALSE}
data("endosulfan")
library("actuar")
fendo.B <- fitdist(endosulfan$ATV, "burr", start = list(shape1 = 0.3, shape2 = 1, rate = 1))
summary(fendo.B)
```

## Why there are differences between MLE and MME for the lognormal distribution?
We recall that the lognormal distribution function is given by
$$
F_X(x) =  \Phi\left(\frac{\log(x)-\mu}{\sigma} \right),
$$
where $\Phi$ denotes the distribution function of the standard normal distribution.
We know that $E(X) = \exp\left( \mu+\frac{1}{2} \sigma^2 \right)$
and $Var(X) = \exp\left( 2\mu+\sigma^2\right) (e^{\sigma^2} -1)$.
The MME is obtained by inverting the previous formulas, whereas
the MLE has the following explicit solution
$$
\hat\mu_{MLE} = \frac{1}{n}\sum_{i=1}^n \log(x_i),~~
\hat\sigma^2_{MLE} = \frac{1}{n}\sum_{i=1}^n (\log(x_i) - \hat\mu_{MLE})^2.
$$
Let us fit a sample by MLE and MME. The fit looks particularly good in both cases.
```{r, fig.height=4, fig.width=8}
x3 <- rlnorm(1000)
f1 <- fitdist(x3, "lnorm", method="mle") 
f2 <- fitdist(x3, "lnorm", method="mme")
par(mfrow=1:2)
cdfcomp(list(f1, f2), do.points=FALSE, xlogscale = TRUE)
denscomp(list(f1, f2), demp=TRUE)
```
Let us compare the theoretical moments (mean and variance) given the fitted values 
($\hat\mu,\hat\sigma$), that is
$$
E(X) = \exp\left( \hat\mu+\frac{1}{2} \hat\sigma^2 \right),
Var(X) = \exp\left( 2\hat\mu+\hat\sigma^2\right) (e^{\hat\sigma^2} -1).
$$
```{r}
c("E(X) by MME"=as.numeric(exp(f2$estimate["meanlog"]+f2$estimate["sdlog"]^2/2)), 
	"E(X) by MLE"=as.numeric(exp(f1$estimate["meanlog"]+f1$estimate["sdlog"]^2/2)), 
	"empirical"=mean(x3))
c("Var(X) by MME"=as.numeric(exp(2*f2$estimate["meanlog"]+f2$estimate["sdlog"]^2)*(exp(f2$estimate["sdlog"]^2)-1)), 
	"Var(X) by MLE"=as.numeric(exp(2*f1$estimate["meanlog"]+f1$estimate["sdlog"]^2)*(exp(f1$estimate["sdlog"]^2)-1)), 
	"empirical"=var(x3))
```
From a MLE point of view, a lognormal sample $x_1,\dots,x_n$ is equivalent to handle 
a normal sample $\log(x_1),\dots,\log(x_n)$.
However, it is well know by the Jensen inequality that 
$E(X) = E(\exp(\log(X))) \geq \exp(E(\log(X)))$ implying the MME estimates provides
better moment estimates than with MLE.



# Questions regarding goodness-of-fits tests and statistics


# Questions regarding optimization procedures

## How to choose optimization method?
If you want to perform optimization without bounds, `optim()` is used.
You can try the derivative-free method Nelder-Mead and the Hessian-free
method BFGS.
If you want to perform optimization with bounds, only two methods are
available without providing the gradient of the objective function:
Nelder-Mead via `constrOptim()` and bounded BFGS via `optim()`.
In both cases, see the help of `mledist()` and the vignette on optimization algorithms.

## The optimization algorithm stops with error code 100. What shall I do?
First, add traces by adding `control=list(trace=1, REPORT=1)`.
Second, try to set bounds for parameters.
Third, find better starting values with `prefit()`.

## Why distribution with a `log` argument may converge better?
Say, we study the shifted lognormal distribution defined by the following density
$$
f(x) = \frac{1}{x \sigma \sqrt{2 \pi}} \exp\left(- \frac{(\ln (x+\delta)- \mu)^2}{2\sigma^2}\right) 
$$
for $x>-\delta$ where $\mu$ is a location parameter, $\sigma$ a scale parameter and 
$\delta$ a boundary parameter.
Let us fit this distribution on the dataset `y` by MLE.
We define two functions for the densities with and without a `log` argument.
```{r}
dshiftlnorm <- function(x, mean, sigma, shift, log = FALSE) dlnorm(x+shift, mean, sigma, log=log)
pshiftlnorm <- function(q, mean, sigma, shift, log.p = FALSE) plnorm(q+shift, mean, sigma, log.p=log.p)
qshiftlnorm <- function(p, mean, sigma, shift, log.p = FALSE) qlnorm(p, mean, sigma, log.p=log.p)-shift
dshiftlnorm_no <- function(x, mean, sigma, shift) dshiftlnorm(x, mean, sigma, shift)
pshiftlnorm_no <- function(q, mean, sigma, shift) pshiftlnorm(q, mean, sigma, shift)
```
We now optimize the minus log-likelihood.
```{r}
data(dataFAQlog1)
y <- dataFAQlog1
D <- 1-min(y)
f0 <- fitdist(y+D, "lnorm")
start <- list(mean=as.numeric(f0$estimate["meanlog"]),  
              sigma=as.numeric(f0$estimate["sdlog"]), shift=D)
# works with BFGS, but not Nelder-Mead
f <- fitdist(y, "shiftlnorm", start=start, optim.method="BFGS")
summary(f)
```
If we don't use the `log` argument, the algorithms stalls.
```{r, error=FALSE}
f2 <- try(fitdist(y, "shiftlnorm_no", start=start, optim.method="BFGS"))
print(attr(f2, "condition"))
```
Indeed the algorithm stops because at the following value, the log-likelihood
is infinite.
```{r}
sum(log(dshiftlnorm_no(y, 0.16383978, 0.01679231, 1.17586600 )))
log(prod(dshiftlnorm_no(y, 0.16383978, 0.01679231, 1.17586600 )))
sum(dshiftlnorm(y, 0.16383978, 0.01679231, 1.17586600, TRUE ))
```
There is something wrong in the computation.

Only the R-base implementation using `log` argument seems reliable.
This happens the C-base implementation of `dlnorm` takes care of the log value.
In the file `../src/nmath/dlnorm.c`  in the R sources, we find the C code for
`dlnorm`
```{r, eval=FALSE, echo=TRUE}
double dlnorm(double x, double meanlog, double sdlog, int give_log)
{
    double y;

#ifdef IEEE_754
    if (ISNAN(x) || ISNAN(meanlog) || ISNAN(sdlog))
	return x + meanlog + sdlog;
#endif
    if(sdlog <= 0) {
	if(sdlog < 0) ML_ERR_return_NAN;
	// sdlog == 0 :
	return (log(x) == meanlog) ? ML_POSINF : R_D__0;
    }
    if(x <= 0) return R_D__0;

    y = (log(x) - meanlog) / sdlog;
    return (give_log ?
	    -(M_LN_SQRT_2PI   + 0.5 * y * y + log(x * sdlog)) :
	    M_1_SQRT_2PI * exp(-0.5 * y * y)  /	 (x * sdlog));
    /* M_1_SQRT_2PI = 1 / sqrt(2 * pi) */

}
```
In the last four lines with the logical condtion `give_log?`, we see how the `log` argument is handled:

* when log=TRUE, we use $-(\log(\sqrt{2\pi}) + y^2/2+\log(x\sigma))$
```{r, eval=FALSE, echo=TRUE}
-(M_LN_SQRT_2PI   + 0.5 * y * y + log(x * sdlog))
```
* when log=FALSE, we use $\sqrt{2\pi} *\exp( y^2/2)/(x\sigma))$ (and then the logarithm outside `dlnorm`)
```{r, eval=FALSE, echo=TRUE}
M_1_SQRT_2PI * exp(-0.5 * y * y)  /	 (x * sdlog))
```
Note that the constant $\log(\sqrt{2\pi})$ is pre-computed in the C macro `M_LN_SQRT_2PI`.


In order to sort out this problem, 
we use the `constrOptim` wrapping `optim` to take into account linear constraints.
This allows also to use other optimization methods than L-BFGS-B 
(low-memory BFGS bounded) used in optim.
```{r}
f2 <- fitdist(y, "shiftlnorm", start=start, lower=c(-Inf, 0, -min(y)), optim.method="Nelder-Mead")
summary(f2)
print(cbind(BFGS=f$estimate, NelderMead=f2$estimate))

```
Another possible would be to perform all computations with higher precision arithmetics as 
implemented in the package `Rmpfr` using the MPFR library.


## What to do when there is a scaling issue?
Let us consider a dataset which has particular small values.
```{r}
data(dataFAQscale1)
head(dataFAQscale1)
summary(dataFAQscale1)
```
The only way to sort out is to multiply the dataset by a large value.
```{r}
for(i in 6:0)
cat(10^i, try(mledist(dataFAQscale1*10^i, "cauchy")$estimate), "\n")
```

Let us consider a dataset which has particular large values.
```{r}
data(dataFAQscale2)
head(dataFAQscale2)
summary(dataFAQscale2)
```
The only way to sort out is to multiply the dataset by a large value.
```{r}
for(i in 0:5)
cat(10^(-2*i), try(mledist(dataFAQscale2*10^(-2*i), "cauchy")$estimate), "\n")
```

## How do I set bounds on parameters when optimizing?


### Setting bounds for scale parameters

Consider the normal distribution $\mathcal{N}(\mu, \sigma^2)$
defined by the density
$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right),
x\in\mathbb{R},
$$
where $\mu$ is a location parameter such that $\mu\in\mathbb{R}$,
$\sigma^2$ is a scale parameter such that $\sigma^2>0$.
Therefore when optimizing the log-likelihood or the squared differences or 
the GoF statistics.
Setting a lower bound for the scale parameter is easy with `fitdist`: just use
the `lower` argument.
```{r scalenormal, echo=TRUE, warning=FALSE}
set.seed(1234)
x <- rnorm(1000, 1, 2)
fitdist(x, "norm", lower=c(-Inf, 0))
```


### Setting bounds for shape parameters

Consider the Burr distribution $\mathcal B(\mu, \sigma^2)$
defined by the density
$$
f(x) = \frac{a b (x/s)^b}{x [1 + (x/s)^b]^{a + 1}},
x\in\mathbb{R},
$$
where $a,b$ are shape parameters such that $a,b>0$,
$s$ is a scale parameter such that $s>0$.
```{r shapeburr, echo=TRUE, warning=FALSE}
x <- rburr(1000, 1, 2, 3)
fitdist(x, "burr", lower=c(0, 0, 0), start=list(shape1 = 1, shape2 = 1, 
  rate = 1))
```


### Setting bounds for probability parameters

Consider the geometric distribution $\mathcal G(p)$
defined by the mass probability function
$$
f(x) = p(1-p)^x,
x\in\mathbb{N},
$$
where $p$ is a probability parameter such that $p\in[0,1]$.
```{r probgeom, echo=TRUE, warning=FALSE}
x <- rgeom(1000, 1/4)
fitdist(x, "geom", lower=0, upper=1)
```


### Setting bounds for boundary parameters

Consider the shifted exponential distribution $\mathcal E(\mu,\lambda)$
defined by the mass probability function
$$
f(x) = \lambda \exp(-\lambda(x-\mu)),
x>\mu,
$$
where $\lambda$ is a scale parameter such that $\lambda>0$,
$\mu$ is a boundary (or shift) parameter such that $\mu\in\mathbb{R}$.
When optimizing the log-likelihood, the boundary constraint is
$$
\forall i=1,\dots,n, x_i>\mu
\Rightarrow
\min_{i=1,\dots,n} x_i > \mu
\Leftrightarrow 
\mu > -\min_{i=1,\dots,n} x_i.
$$
Note that when optimizing the squared differences or the GoF statistics,
this constraint may not be necessary. 
Let us do it in R.
```{r shiftexp, echo=TRUE, warning=FALSE}
dsexp <- function(x, rate, shift)
  dexp(x-shift, rate=rate)
psexp <- function(x, rate, shift)
  pexp(x-shift, rate=rate)
rsexp <- function(n, rate, shift)
  rexp(n, rate=rate)+shift
x <- rsexp(1000, 1/4, 1)
fitdist(x, "sexp", start=list(rate=1, shift=0), lower= c(0, -min(x)))
```

## How works quantile matching estimation for discrete distributions?
Let us consider the Poisson distribution defined 
by the following mass probability
and the cumulative distribution functions
$$
P(X=k)=\frac{\lambda^k}{k!}\exp(-\lambda),~
F_X(x) = \exp(-\lambda)\sum_{k=0}^{\lfloor x \rfloor}\frac{\lambda^k}{k!},~
x\geq 0.
$$
The quantile function $F_X^{-1}(p)=\inf(x, F_X(x)\geq p)$ simplifies to
$$
F_X^{-1}(p) = i \text{ such that } \sum_{k=0}^{i-1} P(X=k) \leq p < \sum_{k=0}^{i} P(X=k).
$$
In other words, this is the following step function
$$
F_X^{-1}(p) =  
\left\{ \begin{array}{ll}
0 & \text{if } p < P(X=0) \\
1 & \text{if } P(X=0) \leq p < P(X=0)+P(X=1) \\
2 & \text{if } P(X=0)+P(X=1) \leq p < P(X=0)+P(X=1)+P(X=2) \\
\dots \\
i & \text{if } \sum_{k=0}^{i-1} P(X=k) \leq p < \sum_{k=0}^{i} P(X=k) \\
\dots \\
\end{array} \right.
$$
The following graphs illustrate this fact.
```{r, fig.height=4, fig.width=8}
ppois(0:3, lambda=1)
qpois(c(0.3, 0.6, 0.9), lambda=1)
par(mar=c(4,4,2,1), mfrow=1:2)
curve(ppois(x, lambda=1), 0, 10, n=301)
curve(qpois(x, lambda=1), 0, 1, n=301)
```

Now we study the QME for the Poisson distribution. Since we have only one parameter,
we choose one probabiliy, $p=1/2$.
The theoretical median is 
$$
F_X^{-1}(1/2) = i \text{ such that } \exp(-\lambda)\sum_{k=0}^{i-1}\frac{\lambda^k}{k!}  \leq 1/2 < \exp(-\lambda)\sum_{k=0}^{i}\frac{\lambda^k}{k!}.
$$
The latter inequality is rewritten as
$$
0 \leq \exp(\lambda)/2 - \sum_{k=0}^{i-1}\frac{\lambda^k}{k!} < \frac{\lambda^i}{i!}
$$
Note that the theoretical median for a discrete distribution is an integer.
Empirically, the median may not be an integer. Indeed for an even length dataset, 
the empirical median is
$$
q_{n,1/2} = \frac{x_{n/2}^\star + x_{n/2+1}^\star}{2},
$$
where $x_{1}^\star<\dots<x_{n}^\star$ is the sorted sample, which is not an integer value 
if $x_{n/2}^\star + x_{n/2+1}^\star$ is not an even number.
However for an odd length dataset, the empirical median is an integer
$q_{n,1/2}=x_{(n+1)/2}^\star$.
```{r}
x <- c(0, 0, 0, 0, 1, 1, 3, 2, 1, 0, 0)
median(x[-1]) #sample size 10
median(x) #sample size 11
```
Therefore, if the median is not an integer, it is impossible to match exactly the 
empirical median with the theoretical quantile.
Furthermore, matching quantiles leads to find some $\lambda$ such that
$$
F_X^{-1}(1/2) = q_{n,1/2}
\Leftrightarrow 
0 \leq \exp(\lambda)/2 - \sum_{k=0}^{q_{n,1/2}-1}\frac{\lambda^k}{k!} < \frac{\lambda^{q_{n,1/2}}}{q_{n,1/2}!}.
$$
Of course, the $\lambda$ is not unique.
Let us plot the squared differences $(F_X^{-1}(1/2) = q_{n,1/2})^2$.
```{r, fig.height=4, fig.width=6}
par(mar=c(4,4,2,1))
x <- rpois(100, lambda=7.5)
L2 <- function(lam)
  (qpois(1/2, lambda = lam) - median(x))^2
curve(L2(x), 6, 9, xlab=expression(lambda), ylab=expression(L2(lambda)), main="squared differences", n=201)
```

Any value between [6.6, 7.6] minimizes the squared differences.
Therefore, using `fitdist()` may be sensitive to the chosen initial value.
```{r}
fitdist(x, "pois", method="qme", probs=1/2, start=list(lambda=2), control=list(trace=1, REPORT=1))
fitdist(x, "pois", method="qme", probs=1/2, start=list(lambda=6.8), control=list(trace=1, REPORT=1))
fitdist(x, "pois", method="qme", probs=1/2, start=list(lambda=15), control=list(trace=1, REPORT=1))
```

Let us consider the geometric distribution with values in $\{0,1,2\dots\}$.
The distribution function and the quantile function are
$$
P(X=x)= p (1-p)^{\lfloor x\rfloor},
F_X(x) = 1- (1-p)^{\lfloor x\rfloor},
F_X^{-1}(q) = \left\lfloor\frac{\log(1-q)}{\log(1-p)}\right\rfloor.
$$
Again the squared difference is a step function.
```{r, fig.height=4, fig.width=6}
x <- rgeom(100, 1/3)
L2 <- function(p)
  (qgeom(1/2, p) - median(x))^2
curve(L2(x), 0.10, 0.95, xlab=expression(p), ylab=expression(L2(p)), main="squared differences", n=301)
```
```{r}
median(x)
L2(1/3)
fitdist(x, "geom", method="qme", probs=1/2, start=list(prob=1/2), control=list(trace=1, REPORT=1))
fitdist(x, "geom", method="qme", probs=1/2, start=list(prob=1/20), control=list(trace=1, REPORT=1))
```



